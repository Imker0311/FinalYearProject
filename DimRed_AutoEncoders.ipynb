{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6c3263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset 'C_A' with shape (1800000, 1)\n",
      "Loaded dataset 'T' with shape (1800000, 1)\n",
      "Loaded dataset 'T_C' with shape (1800000, 1)\n",
      "Loaded dataset 'h' with shape (1800000, 1)\n",
      "Loaded dataset 'Q_vec' with shape (1800000, 1)\n",
      "Loaded dataset 'Q_c_vec' with shape (1800000, 1)\n",
      "Loaded dataset 'l' with shape (1800000, 1)\n",
      "Loaded dataset 'Q_SP' with shape (1800000, 1)\n",
      "Loaded dataset 'l_c' with shape (1800000, 1)\n",
      "Loaded dataset 'Qc_SP' with shape (1800000, 1)\n",
      "Loaded dataset 'F4' with shape (1800001, 1)\n",
      "Loaded dataset 'F5' with shape (1800001, 1)\n",
      "Loaded dataset 'F6' with shape (1800001, 1)\n",
      "Loaded dataset 'F7' with shape (1800001, 1)\n",
      "Loaded dataset 't' with shape (1800001,)\n",
      "Loaded dataset 'F1.plt' with shape (1800001,)\n",
      "Loaded dataset 'F2.plt' with shape (1800001,)\n",
      "Loaded dataset 'F3.plt' with shape (1800001,)\n",
      "Loaded dataset 'F4.plt' with shape (1800001,)\n",
      "Loaded dataset 'F5.plt' with shape (1800001,)\n",
      "Loaded dataset 'F6.plt' with shape (1800001,)\n",
      "Loaded dataset 'F7.plt' with shape (1800001,)\n",
      "Loaded dataset 'F8.plt' with shape (1800001,)\n",
      "Loaded dataset 'F9.plt' with shape (1800001,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tensorflow import keras\n",
    "from keras import layers, Model, optimizers, Input\n",
    "\n",
    "def load_vector(filename, name):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        data = f[name][:]\n",
    "        print(f\"Loaded dataset '{name}' with shape {data.shape}\")\n",
    "        return data\n",
    "\n",
    "def save_vector(filename, name, data): # Creates dataset if it does not exist, overwrites if it does\n",
    "    with h5py.File(filename, 'a') as f:  # open file in append mode\n",
    "        if name in f:\n",
    "            del f[name]  # delete old dataset before overwriting\n",
    "        f.create_dataset(name, data=data, chunks=True, compression='gzip')\n",
    "        print(f\"Saved dataset '{name}' with shape {data.shape}\")\n",
    "\n",
    "file = 'CSTR_Simulation.h5'\n",
    "\n",
    "C_A = load_vector(file, 'C_A')\n",
    "T = load_vector(file, 'T')\n",
    "T_C = load_vector(file, 'T_C')\n",
    "h = load_vector(file, 'h')\n",
    "Q = load_vector(file, 'Q_vec')\n",
    "Q_C = load_vector(file, 'Q_c_vec')\n",
    "\n",
    "l = load_vector(file, 'l')\n",
    "Q_SP = load_vector(file, 'Q_SP')\n",
    "l_C = load_vector(file, 'l_c')\n",
    "Q_C_SP = load_vector(file, 'Qc_SP')\n",
    "\n",
    "file = 'TestInputVectorsMinutes.h5'\n",
    "\n",
    "T_F = load_vector(file, 'F4')\n",
    "C_AF = load_vector(file, 'F5')\n",
    "T_CF = load_vector(file, 'F6')\n",
    "Q_F = load_vector(file, 'F7')\n",
    "\n",
    "t = load_vector(file, 't')\n",
    "\n",
    "F1 = load_vector(file, 'F1.plt').reshape(-1)*1\n",
    "F2 = load_vector(file, 'F2.plt').reshape(-1)*2\n",
    "F3 = load_vector(file, 'F3.plt').reshape(-1)*3\n",
    "F4 = load_vector(file, 'F4.plt').reshape(-1)*4\n",
    "F5 = load_vector(file, 'F5.plt').reshape(-1)*5\n",
    "F6 = load_vector(file, 'F6.plt').reshape(-1)*6\n",
    "F7 = load_vector(file, 'F7.plt').reshape(-1)*7\n",
    "F8 = load_vector(file, 'F8.plt').reshape(-1)*8\n",
    "F9 = load_vector(file, 'F9.plt').reshape(-1)*9\n",
    "\n",
    "T_F = T_F[1:]\n",
    "C_AF = C_AF[1:]\n",
    "T_CF = T_CF[1:]\n",
    "Q_F = Q_F[1:]\n",
    "t = t[1:]\n",
    "F1 = F1[1:]\n",
    "F2 = F2[1:]\n",
    "F3 = F3[1:]\n",
    "F4 = F4[1:]\n",
    "F5 = F5[1:]\n",
    "F6 = F6[1:]\n",
    "F7 = F7[1:]\n",
    "F8 = F8[1:]\n",
    "F9 = F9[1:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a2a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800000, 1)\n",
      "Fault 0: 1668600 occurrences\n",
      "Fault 1: 8880 occurrences\n",
      "Fault 2: 31680 occurrences\n",
      "Fault 3: 4200 occurrences\n",
      "Fault 5: 32520 occurrences\n",
      "Fault 6: 21600 occurrences\n",
      "Fault 7: 4320 occurrences\n",
      "Fault 8: 10200 occurrences\n",
      "Fault 9: 10200 occurrences\n",
      "Fault 10: 7800 occurrences\n"
     ]
    }
   ],
   "source": [
    "faults = np.vstack([F1,F2,F3,F4,F5,F6,F7,F8,F9])\n",
    "\n",
    "def combine_fault_vectors(faults):\n",
    "\n",
    "    # For each time step, count how many faults are active (non-zero)\n",
    "    active_counts = np.count_nonzero(faults, axis=0)\n",
    "    \n",
    "    # Initialize combined vector with zeros\n",
    "    combined = np.zeros(faults.shape[1], dtype=int)\n",
    "    \n",
    "    # Find indices where exactly one fault is active\n",
    "    single_fault_idx = np.where(active_counts == 1)[0]\n",
    "    \n",
    "    # For these indices, set combined to the fault number (non-zero value)\n",
    "    # Since only one fault active, sum will give the fault number\n",
    "    combined[single_fault_idx] = faults[:, single_fault_idx].sum(axis=0)\n",
    "    \n",
    "    # For indices where multiple faults active, set combined to 10\n",
    "    multiple_fault_idx = np.where(active_counts > 1)[0]\n",
    "    combined[multiple_fault_idx] = 10\n",
    "    \n",
    "    return combined\n",
    "\n",
    "Fault_class = combine_fault_vectors(faults).reshape(-1, 1)\n",
    "\n",
    "print(Fault_class.shape)\n",
    "\n",
    "values, counts = np.unique(Fault_class, return_counts=True)\n",
    "\n",
    "for val, count in zip(values, counts):\n",
    "    print(f\"Fault {val}: {count} occurrences\")\n",
    "\n",
    "df_full = np.hstack([Fault_class, C_A, T, T_C, h, Q, Q_C, Q_F, C_AF, T_F, T_CF, l, Q_SP, l_C, Q_C_SP])\n",
    "df = df_full[::10, :]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(df[df[:, 0] == 0, 1:])\n",
    "df_scaled =sc.transform(df[:, 1:])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled, df[:, 0], test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3ba5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoding_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m150\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoding_layer (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m28\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m30\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m70\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m154\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">498</span> (1.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m498\u001b[0m (1.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">498</span> (1.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m498\u001b[0m (1.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 593us/step - loss: 5.1721 - val_loss: 5.1440\n",
      "Epoch 2/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 561us/step - loss: 5.1987 - val_loss: 5.1368\n",
      "Epoch 3/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 561us/step - loss: 5.1774 - val_loss: 5.1318\n",
      "Epoch 4/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 552us/step - loss: 4.9446 - val_loss: 5.1287\n",
      "Epoch 5/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558us/step - loss: 5.0997 - val_loss: 5.1264\n",
      "Epoch 6/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 546us/step - loss: 5.1357 - val_loss: 5.1368\n",
      "Epoch 7/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 533us/step - loss: 5.4242 - val_loss: 5.1297\n",
      "Epoch 8/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 536us/step - loss: 5.2380 - val_loss: 5.1326\n",
      "Epoch 9/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 546us/step - loss: 5.5557 - val_loss: 5.1331\n",
      "Epoch 10/10\n",
      "\u001b[1m3938/3938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 543us/step - loss: 5.1836 - val_loss: 5.1325\n",
      "Test loss: 5.132528781890869\n"
     ]
    }
   ],
   "source": [
    "def build_autoencoder(\n",
    "    input_dim,\n",
    "    encoding_dim,\n",
    "    hidden_layers=[64, 32],  # List of units for hidden layers before bottleneck\n",
    "    activation='relu',\n",
    "    output_activation='sigmoid',\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    loss='mse'\n",
    "):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder\n",
    "    x = input_layer\n",
    "    for units in hidden_layers:\n",
    "        x = layers.Dense(units, activation=activation)(x)\n",
    "    encoded = layers.Dense(encoding_dim, activation=activation, name='encoding_layer')(x)\n",
    "\n",
    "    # Decoder (mirror of encoder)\n",
    "    x = encoded\n",
    "    for units in reversed(hidden_layers):\n",
    "        x = layers.Dense(units, activation=activation)(x)\n",
    "    output_layer = layers.Dense(input_dim, activation=output_activation)(x)\n",
    "\n",
    "    # Autoencoder model: input to reconstructed output\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Encoder model: input to bottleneck representation\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    # Set optimizer\n",
    "    opt_lower = optimizer.lower()\n",
    "    if opt_lower == 'adam':\n",
    "        opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif opt_lower == 'sgd':\n",
    "        opt = optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif opt_lower == 'rmsprop':\n",
    "        opt = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif opt_lower == 'adagrad':\n",
    "        opt = optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer selected.\")\n",
    "\n",
    "    # Compile autoencoder\n",
    "    autoencoder.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    # Return both models\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Build autoencoder model\n",
    "autoencoder, encoder = build_autoencoder(\n",
    "    input_dim=14,\n",
    "    encoding_dim=4,\n",
    "    hidden_layers=[10,6],  # Example hidden layers\n",
    "    activation='relu',\n",
    "    output_activation='sigmoid',\n",
    "    optimizer='rmsprop',\n",
    "    learning_rate=0.01,\n",
    "    loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# Autoencoder training\n",
    "history = autoencoder.fit(\n",
    "    X_train,\n",
    "    X_train,  # Autoencoders use input as output\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, X_test),\n",
    "    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "print(f\"Test loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c36d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207us/step\n"
     ]
    }
   ],
   "source": [
    "compressed_data = encoder.predict(df_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[116533     14     78      0     22    104      1      0      0      0]\n",
      " [   553     82      7      0      0      0      0      0      0      0]\n",
      " [  1786      0    418      0      0      4      0      0      0      3]\n",
      " [    27      0      0    268      0      0      0      0      0      0]\n",
      " [  1049      0      0      0   1282      1      0      0      0      0]\n",
      " [   983      0      0      0      0    500      0      0      0      0]\n",
      " [   177      0      0      0      0      0    130      0      0      0]\n",
      " [   233      0      0      0      0      0      0    482      0      0]\n",
      " [   720      0      0      0      0      0      0      0      1      0]\n",
      " [    42      0      9      0      0      0      0      0      0    491]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98    116752\n",
      "         1.0       0.85      0.13      0.22       642\n",
      "         2.0       0.82      0.19      0.31      2211\n",
      "         3.0       1.00      0.91      0.95       295\n",
      "         5.0       0.98      0.55      0.71      2332\n",
      "         6.0       0.82      0.34      0.48      1483\n",
      "         7.0       0.99      0.42      0.59       307\n",
      "         8.0       1.00      0.67      0.81       715\n",
      "         9.0       1.00      0.00      0.00       721\n",
      "        10.0       0.99      0.91      0.95       542\n",
      "\n",
      "    accuracy                           0.95    126000\n",
      "   macro avg       0.94      0.51      0.60    126000\n",
      "weighted avg       0.95      0.95      0.94    126000\n",
      "\n",
      "Saved dataset 'y_pred' with shape (126000,)\n",
      "Saved dataset 'y_test' with shape (126000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(compressed_data, df[:, 0], test_size=0.7)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel='rbf', C=1.0, gamma= 1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = classifier.predict(X_test)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "file = 'Classification_Results.h5'\n",
    "\n",
    "save_vector(file, 'y_pred', y_pred)\n",
    "save_vector(file, 'y_test', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
